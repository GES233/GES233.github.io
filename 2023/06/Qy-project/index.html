<html lang="zh"><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="description" content=""/><meta name="keyword"/><title>Qy 计划的介绍
-
自我放逐之地
-

记录学习生活的大小事</title><link rel="icon" href="/img/favicon.ico"/>
<link rel="stylesheet" href="/css/style.css">

<link rel="stylesheet" href="/css/helpers.css">

<script src="/js/clipboard/clipboard.min.js"></script>


<script src="/js/bootstrap.js"></script>

<script async="async" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><meta name="generator" content="Hexo 7.2.0"></head><body><div class="loading-wrapper" data-loading="data-loading"><div class="loading"><span></span><span></span><span></span></div></div><div class="page" data-filter="data-filter"><div class="head" data-show="data-show"><header class="head-header"><div class="head-author"><a class="head-author-link" href="/">自我放逐之地</a></div><div class="head-right"><button class="bar-wrap" id="bar-wrap-toggle"><span class="bar"></span><span class="bar"></span><span class="bar"></span></button><div class="head-item"><a class="search-button head-item-link"><span>搜索</span>
<i class="icon icon-search"></i></a></div><div class="head-item"><a class="head-item-link" href="/about">是什么？</a><a class="head-item-link" href="/">　</a><a class="head-item-link" href="/2023/07/Intro/">为什么？</a></div></div></header>
<div class="menubar-head" id="menubar"><ul class="menubar-ul"><li class="menubar-item"><i class="icon icon-chevron-right"></i>
<a class="menubar-link" href="/categories/%E4%BB%A5%E5%89%8D/">旧事重提</a></li><li class="menubar-item"><i class="icon icon-chevron-right"></i>
<a class="menubar-link" href="/categories/%E7%95%9C%E7%89%A7/">关于养猪羊牛马只因</a></li><li class="menubar-item"><i class="icon icon-chevron-right"></i>
<a class="menubar-link" href="/categories/%E4%B8%93%E4%B8%9A%E8%AF%BE/">啊，他妈的</a></li><li class="menubar-item" data-border="data-border"></li><li class="menubar-item"><i class="icon icon-archive"></i>
<a class="menubar-link" href="/archives">归档</a></li><li class="menubar-item"><i class="icon icon-tags"></i>
<a class="menubar-link" href="/tags">标签</a></li><li class="menubar-item" data-border="data-border"></li><li class="menubar-item"><a class="menubar-link" href="/about"><span>是什么？</span></a></li><li class="menubar-item"><a class="menubar-link" href="/"><span>　</span></a></li><li class="menubar-item"><a class="menubar-link" href="/2023/07/Intro/"><span>为什么？</span></a></li></ul><div class="menu-search-box search-button"><div>搜索</div>
<i class="icon icon-search"></i></div></div></div><div class="main" data-page="post"><article class="post" id="post"><header class="post-head"><h1 class="post-title"><a class="title" href="/2023/06/Qy-project/">Qy 计划的介绍</a></h1></header><div class="post-meta"><div class="post-date"><time class="post-time" itemprop="datePublished" title="2023-06-23 20:00:27" datetime="2023-06-23T12:00:27.000Z">2023-06-23</time></div>|
<div class="post-tag"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E4%BB%8B%E7%BB%8D/" rel="tag">介绍</a></li></ul></div>
<div class="post-visit"><span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span>访问</span></div></div><div class="post-info"><div class="post-word-count">本文共2,346字。阅读完需要约8分钟。</div>
<div class="post-cc">版权声明：署名-非商业性使用-相同方式共享 4.0 国际

|
<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-hans">CC BY-NC-SA 4.0</a></div></div><div class="article-entry" itemprop="articleBody"><p>Qy 是从 18 至 19 年某几次头脑风暴催生出的的计划，即通过某种仿生的方式来使角色生成活灵活现的动作（包括但不限于声音、面部表情或身体上的动作）。</p>
<span id="more"></span>

<p><del>人话：做个会动会唱歌有表情的小姐姐。</del></p>
<h3 id="早期想法"><a href="#早期想法" class="headerlink" title="早期想法"></a>早期想法</h3><p>最原始的想法是想通过某种方法来尽可能的实现一个能够覆盖人类发声器官所能出现的绝大多数的声音的模型。</p>
<blockquote><p>之前在看语音合成的相关资料的时候，想出了一个通过重现发声而不是对音频进行处理来生成音频的思路，并且试图再深入想了一下，但是因为对发声同时其他数据的采集的难度而暂时放弃。然后，今天刷知乎的时候，发现有人已经按照这个思路搞出来并且把论文发到 Nature 了，而且比我的想法更进一步：通过对发声相关的运动皮质的信号的采集，通过两个 RNN 模型（声道参数为其中间参量，其生成使用了其他大量的数据库）生成语音。生成的音质及准确率较以往的其他模型要好。</p>
<footer><strong>来自笔者的某条说说 2019年4月27日 22:29</strong></footer></blockquote>

<p>可以由此看出，最早的版本，仅仅局限于提出一种新的语音合成的思路罢了。按照原理来讲，它具有跨语言以及表达丰富且细腻的感情的潜能。</p>
<p>在高中某天参加社团百无聊赖的时候（其实在我写上面那则说说之前），我忽然意识到这种中间状态的思路可以产生很丰富的表现力。就比方说：你完全可以从话语变成语音以及此刻发声器官的样貌、舌头的位置、胸廓的张开程度等等方面的内容。换言之，其会拥有更加细腻的表达能力。</p>
<p>同时，也需要考虑到在模型的表征与展示的形式不可能是同一个（就比方说出现在屏幕与 AR 中这类不同的场景），所以需要定义一系列的更加抽象的标准来去实现。</p>
<p>因此这个没有成形的想法就一直没有丢掉，后面也随着我的学识的增长慢慢完善。</p>
<h4 id="关于名字"><a href="#关于名字" class="headerlink" title="关于名字"></a>关于名字</h4><p>分成两部分说吧。</p>
<p>It startwith <code>Q</code> and endwith <code>y</code>.</p>
<p>前者主要是受一个忘了名字的老哥（字母哥）的影响。刚上外网的时候，那时候是 15 年，对计算机组成原理很感兴趣（原因是想用 Minecraft 的红石做一个可以跑的电脑出来）。刷油管看到个 Logisim CPU（用数字电路模拟软件整的），有个老哥用这玩意儿做演示创建文件的时候就说：</p>
<blockquote>
<p>This is a text file called “Q”.</p>
</blockquote>
<p>挺好玩的，就用上了。</p>
<p>后者是不想要太过张扬的名字，想整个闭口呼<a href="%E6%B1%89%E8%AF%AD%E9%9F%B3%E9%9F%B5%E5%AD%A6%E4%B8%AD%EF%BC%8C%E6%9C%89%E5%9B%9B%E5%91%BC%EF%BC%8C%E5%88%86%E5%88%AB%E6%8C%87%E5%BC%80%E5%8F%A3%E5%91%BC%E3%80%81%E9%BD%90%E9%BD%BF%E5%91%BC%E3%80%81%E5%90%88%E5%8F%A3%E5%91%BC%E5%92%8C%E6%92%AE%E5%8F%A3%E5%91%BC%E3%80%82">^1</a>的发音。就敲定了 <code>i</code> 或 <code>y</code> 。</p>
<p><code>Qi</code> 这个词已经有了，大概是「气」的音译。但是这个项目（或者说，企划）和气没什么关系。</p>
<p>就选另一个字母了。</p>
<p>那怎么念呢？</p>
<p>别人怎么念我不管，没什么是必须的。</p>
<p>可以拼出来读，也可以按照qui-&#x2F;ki-（音标[kʰwɪ]&#x2F;[kʷɪ]&#x2F;[kɪ]）来发音，或者可以按照汉语拼音的 qí（注音［ㄑㄧˊ］）来念。</p>
<h3 id="核心思路"><a href="#核心思路" class="headerlink" title="核心思路"></a>核心思路</h3><p><em>用自然的原理来重现某些生理特征的思路在结果上是自然的，方法本身也具有可实现性。</em></p>
<p>说白了就是尽量精细且精确的仿生过程产生出非常自然细腻精细的动作。</p>
<p><strong>四大支柱</strong>：</p>
<ul>
<li>就旁观者而言，动作的产生是由<strong>施加动作的主体与环境的互动</strong>所产生的结果<ul>
<li>主体给予的是要执行动作时的「指令」，例如一系列相关运动神经元的放电</li>
<li>以「文本转语音」（TTS）为例，主体与环境的互动指的是声带与气流的作用</li>
</ul>
</li>
<li>其中主体与环境的互动的部分机制<strong>难以<em>完全</em>复现</strong><ul>
<li>相关算法的研究进展以及数据采集的难度，限制了在计算机上完全地仿真主体与环境交互的能力</li>
</ul>
</li>
<li>动作的被施加存在<strong>意识可控</strong>和<strong>不可控</strong>的成分<ul>
<li>意识无法极其精确地控制运动，意识也难以使主体完成其能力之外的动作，有道是「人力有穷尽，力所不能及」</li>
</ul>
</li>
<li>需要施加动作主体的<strong>当前状态</strong>作为感觉<ul>
<li>对应着上文的「互动」</li>
</ul>
</li>
</ul>
<h4 id="人话版本"><a href="#人话版本" class="headerlink" title="人话版本"></a>人话版本</h4><p>上面那一堆确实不大好懂，已经让 ChatGPT 翻译过的还是很不像人话。</p>
<blockquote><p>拿最早想通过这种思路来实现文本转语音的例子来说的话，我会在传统的TTS的基础上，在语言学特征与音频之间添加“动作”这个中间参数，产生的音频则是动作下个体与环境的互动（呼气时收紧声带，气流与声带的相互作用产生音频），然后将动作与环境的信息（在这里是肌肉收缩的状态、本体感觉以及发出的声音）作为某种感觉对下一刻的动作进行反馈。</p>
<p>但是和现有技术不同的是，因为主体与环境的互动过程难以复现（还是以文本转语音为例，通过每刻的声道状态来对声音进行仿真及其消耗算力），我们将这些交给对应的机器学习模型或是经过极致优化的算法解决，因此该部分属于“可插拔的”。</p>
<p>我们的原型设计通过构建离体喉模型开始，通过体外的电信号刺激以及通过离体喉的气流来实现对该发声部位的控制。记录刺激以及数据来参与模型（主要是动作-输出模型，在这里打算采用 WaveNet）的训练（按照组织活性的保持时间，更可能是对现有模型进行微调）。下一步是根据解剖学以及生理学的知识，了解喉相关神经的放电模式与运动的关系，再通过构建、采集数据以及训练，进而得到（指令-动作）模型。和最开始计划有所差异的是，因为其仅仅作为验证可行性的原型，并没有考虑反馈的部分。</p>
<footer><strong>部分来自我给出的 prompt</strong></footer></blockquote>

<p>所以再在后面尝试用大白话逼叨逼叨。</p>
<p><del>和上面我给 ChatGPT 的 prompt 一样，我想要通过一系列存在关联的例子来阐明。</del></p>
<h5 id="作为仿生的控制回路"><a href="#作为仿生的控制回路" class="headerlink" title="作为仿生的控制回路"></a>作为仿生的控制回路</h5><p>…</p>
<h5 id="作为面向动作的平台"><a href="#作为面向动作的平台" class="headerlink" title="作为面向动作的平台"></a>作为面向动作的平台</h5><p>简单来说就是确定一系列通用的协议来定义角色（avatar）的动作的范畴。需要重新讨论是因为动作包括本身不受角色的特殊属性的影响的部分（比方说基本的抬手放下）以及需要这些特殊属性的部分（例如反手摸肚脐之类的）。</p>
<p>对相关从业者来说，将原本可能需要重新设计的动作解耦到不同的层级，可以显著的减少工作量。</p>
<h3 id="SynapticStrings-是啥？"><a href="#SynapticStrings-是啥？" class="headerlink" title="SynapticStrings 是啥？"></a>SynapticStrings 是啥？</h3><p>原来叫 <code>Generic-Qy</code> ，换了这个更酷的名字。</p>
<p>整这出是因为笔者在这个夏天吃了不少虚拟歌姬圈子的瓜（<del>说句难听点的，小圈子逼事儿真多</del>）。而且在一些群里潜水也发现很有一种B站「开发者」群体的那种小孩子过家家的感觉或者叫特色。</p>
<p>所以选择将原来的 Qy 拆分成两个内容，Qy 继续面向 ACGN ，更偏向于应用领域；另一个是纯粹理论与技术（研究针对哺乳动物的通用模型），与 Qy 受众不同，可能比较「枯燥」。</p>
<p>说实话，两拨人（指搞技术的与和搞艺术的）本来就尿不到一壶去，能和睦相处甚至是都会的真不多，与其聚一块儿养蛊还不如分开。</p>
<p>各自专注于技术与内容生产，倒也算是安稳。</p>
<p>说话来，这个原来的计划是打算通过动物模型来构建原型以测试整个思路的可行性。</p>
<p>就是原型验证加上相关的子计划。</p>
<h3 id="子计划"><a href="#子计划" class="headerlink" title="子计划"></a>子计划</h3><p>注：绝大多数计划可能迁移到 <code>SynapticStrings</code> 中， <code>Qy</code> 更加专注 ACGN 领域本身。</p>
<table>
<thead>
<tr>
<th align="center">名字</th>
<th align="center">仓库名&#x2F;包名</th>
<th align="center">语言</th>
<th align="left">描述</th>
<th align="left">进度</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Vivid Puppet</td>
<td align="center"><code>vivid-puppet</code></td>
<td align="center">Elixir</td>
<td align="left">框架本身</td>
<td align="left">设计 &amp; slide</td>
</tr>
<tr>
<td align="center">NSP (Neural Simulate Platform)</td>
<td align="center"><code>TaroPaste</code></td>
<td align="center">Elixir &amp; Rust</td>
<td align="left">提供可扩展的事件驱动平台，使其实现仿生的指令</td>
<td align="left">原型(<a target="_blank" rel="noopener" href="https://github.com/GES233/Glowworm">Glowworm</a>)</td>
</tr>
<tr>
<td align="center">Visual Throat</td>
<td align="center">&#x2F;</td>
<td align="center">Python</td>
<td align="left">基于体外咽喉研究平台构建出的实时声音生成模型</td>
<td align="left">计划 &amp; 无限期暂停（专业限制）</td>
</tr>
<tr>
<td align="center">TTV</td>
<td align="center"><code>text2voice</code></td>
<td align="center">Python, Julia</td>
<td align="left">文本转声音，依赖于 NSP 以及 VisualThroat</td>
<td align="left">计划</td>
</tr>
<tr>
<td align="center">Cheese Meow</td>
<td align="center"><code>CheeseMeow</code></td>
<td align="center">未定</td>
<td align="left">情感计算与仿真（Emotion Emulation）</td>
<td align="left">计划 &amp; 可行性分析</td>
</tr>
</tbody></table>
<h3 id="其他内容"><a href="#其他内容" class="headerlink" title="其他内容"></a>其他内容</h3><ul>
<li><a href="/pdf/web/viewer.html?file=/pdf/QySlide.pdf">今年早期的文档</a></li>
</ul>
<p>——暂时完结——</p>
<h3 id="comments">留言评论</h3><div id="gitalk-comments"></div><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"/><script src="https://rawgit.com/qhh0205/78e9e0b1f3114db6737f3ed8cdd51d3a/raw/3894c5be5aa2378336b1f5ee0f296fa0b22d06e9/md5.min.js"> </script><script src="//unpkg.com/gitalk/dist/gitalk.min.js"></script><script>window.gitalk = new Gitalk({
  clientID: 'Ov23lix7MuO7I04fa5Jd',
  clientSecret: 'b992088340e08af51f4ec54880194f2bbca1a155',
  repo: 'GES233.github.io',
  owner: 'GES233',
  admin: 'GES233'.split(','),
  id: md5(location.pathname),
  distractionFreeMode: false  // Facebook-like distraction free mode
});
window.gitalk.render('gitalk-comments'); // Render Gitalk in the element with ID 'gitalk-container'</script></div></article><aside class="post-widget"><h4>目录</h4><nav class="post-toc-wrap" id="post-toc"><ol class="post-toc"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E6%97%A9%E6%9C%9F%E6%83%B3%E6%B3%95"><span class="post-toc-number">1.</span> <span class="post-toc-text">早期想法</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E5%85%B3%E4%BA%8E%E5%90%8D%E5%AD%97"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">关于名字</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E8%B7%AF"><span class="post-toc-number">2.</span> <span class="post-toc-text">核心思路</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E4%BA%BA%E8%AF%9D%E7%89%88%E6%9C%AC"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">人话版本</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#%E4%BD%9C%E4%B8%BA%E4%BB%BF%E7%94%9F%E7%9A%84%E6%8E%A7%E5%88%B6%E5%9B%9E%E8%B7%AF"><span class="post-toc-number">2.1.1.</span> <span class="post-toc-text">作为仿生的控制回路</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#%E4%BD%9C%E4%B8%BA%E9%9D%A2%E5%90%91%E5%8A%A8%E4%BD%9C%E7%9A%84%E5%B9%B3%E5%8F%B0"><span class="post-toc-number">2.1.2.</span> <span class="post-toc-text">作为面向动作的平台</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#SynapticStrings-%E6%98%AF%E5%95%A5%EF%BC%9F"><span class="post-toc-number">3.</span> <span class="post-toc-text">SynapticStrings 是啥？</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E5%AD%90%E8%AE%A1%E5%88%92"><span class="post-toc-number">4.</span> <span class="post-toc-text">子计划</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E5%85%B6%E4%BB%96%E5%86%85%E5%AE%B9"><span class="post-toc-number">5.</span> <span class="post-toc-text">其他内容</span></a></li></ol></nav></aside></div><footer class="footer-nav"><div class="footer"><div class="back-top" id="back-top" title="Back to top"><i class="icon icon-chevron-bar-up"></i></div><div class="footer-content"><div><span id="busuanzi_container_site_pv"><span id="busuanzi_value_site_pv">?</span>
PV
</span><span id="busuanzi_container_site_uv"><span id="busuanzi_value_site_uv">?</span>
UV</span></div>

Copyright &copy;
2023<span class="time-divide">-</span>2024
GES233.

Power by
<a href="https://hexo.io/" target="_blank" rel="external nofollow">Hexo</a>
and
<a href="https://github.com/Cerallin/hexo-theme-yuzu" target="_blank" rel="external nofollow" title="v3.2.4">Theme Yuzu</a>.</div></div></footer>
<script>window.config = {
  url_root: '/',
  meta_path: 'meta.json',
};
</script>
<script src="/js/theme/back-to-top.js"></script>


<script src="/js/theme/clipboard.js"></script>


<script src="/js/theme/loading.js"></script>


<script src="/js/theme/navbar.js"></script>

<script src="/js/theme/search.js"></script>

<script src="/js/theme/toc.js"></script>
<script>window.onload = function () {
  for (const moduleName in Theme) {
    const module = Theme[moduleName];
    module.register();
  }
};</script></div><div class="search-modal" id="search-modal"><div class="card"><div class="card-head"><div class="search-box"><input class="search-input" id="search-input" placeholder="搜索"/><div class="search-button" id="search-button"><div class="icon icon-search"></div></div></div><div class="close-button"><div class="icon icon-x"></div></div></div><div class="card-body"><div class="search-count">共<span id="search-count-num">0</span>条搜索结果。</div><div class="search-result" id="search-result"></div></div></div></div></body></html>